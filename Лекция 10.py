# Линейная регрессия

# Задача: на основе наблюдаемых точек построить прямую, которая отображает связь между двумя или более переменными.abs
# Регрессия пытается подогнать некоторую функцию к наблюдаемым данным, чтобы спрогнозировать новые данные
# Линейная регрессия подгоняем данные к прямой линии, пытаемся установить линейную связь между переменными и предсказать новые данные

import numpy as np
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from numpy.linalg import inv, qr
import random
features, target = make_regression(n_samples=100, n_features=1, n_informative=1, n_targets=1, noise=1, random_state=1)

print(features.shape)
print(target.shape)

model = LinearRegression().fit(features, target)

#plt.scatter(features, target)

#x = np.linspace(features.min(), features.max(), 100)
# y = kx+b
#plt.plot(x, model.coef_[0] * x + model.intercept_, color='red')

#plt.show()

# Простая линейная регрессия

# Линейная -> линейная зависимость.

# + прогнозирование на новых данных
# + анализ взаимного влияния переменных друг на друга

# - точки обучаемых данных НЕ будут точно лежать на прямой (из-за щума) => область погрешности
# - линейная регрессия не позволяет делать прогнозы ВНЕ диапазона имеющихся данных

# Данные, на основании которых разрабатывается модель - это выборка из совокупности, хотелось бы, чтобы это была репрезентативная выборка

# Опр - остатки(отклонения, ошибки) - расстояния между точками данных и ближайшими по вертикали точками на прямой

# y = w0 + w1*x
# Задача состоит в минимизации остатков => разрыв между прямой и точками будет минимальным

# чаще всего используют сумму квадратов остатков - минимизируем сумму площадей квадратов (обучение модели - минимизация функции потерь)

# Решение: Аналитическое решение и численное решение
# Численные методы проще и доступнее, но требуют большей вычисл мощности, дает решение с погрешностью
# Аналитические - правильные и точные, но их иногда сложно/невозможно получить

# На вход (xi, yi)

data = np.array(
    [
        [1, 5],
        [2, 7],
        [3, 7],
        [4, 10],
        [5, 11],
        [6, 14],
        [7, 17],
        [8, 19],
        [9, 22],
        [10, 28],
    ]

)
x= data[:, 0]
y = data[:, 1]

n = len(x)
w_1 = (n*sum(x[i]*y[i] for i in range(n)) - sum(x[i] for i in range (n)) * sum(y[i] for i in range (n))) / (n * sum(x[i] ** 2 for i in range(n)) - sum(x[i] for i in range(n)) ** 2)
w_0 = (sum((y[i]) for i in range(n)))/ n-w_1 * (sum(x[i] for i in range(n))) / n

print(w_1, w_0)

#2.4 0.8

# 2. Метод обратных матриц
# y = w_1*x + w_0;  w(vector) = [w_0, w_1]
# X = [1 x_1, ..., 1 x_n]; y = [y1, y2, ..., yn]
# w = (X^T * X)^(-1) * X^T*y

x_1 = np.vstack([x, np.ones(len(x))]).T
w = inv(x_1.transpose() @ x_1) @ (x_1.transpose() @ y)
print(w)

#3. Разложение матриц
# X = Q*R -> w = R^(-1) * Q^T * y (QR разложение) приближ вычисление - помогает минимизировать ошибки в вычислениях

Q,R = qr(x_1)
w = inv(R).dot(Q.transpose()).dot(y)

print(w)

#4. Градиентный способ
# Метод оптимизации, где используются производные и итерации
# Частные производные по одному из параметров. Позволяет определить угловой коэффициент и изменение параметра выполняется в ту сторону, где он максимален/минимален
# Для бОльших угловых коээф делается более широкий "шаг". Ширина шага обычно вычисляется как доля от углового коэфф , связано со скоростью обучения
# Чем выше скорость, тем быстрее будет работать система. Это делается засчет снижения точности. Чем ниже скорость, тем больше времни займет обучение, но точность будет выше.

# f(x) = (x-3)^2 + 4 Найти х, где f(x) минимальна

def f(x):
    return (x-3) ** 2 + 4

def dx_f(x):
    return 2*x - 6

# x = np.linspace(-10,10,100)
#
# ax = plt.gca()
#
# ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))
#
#
# #plt.plot(x, f(x))
# plt.plot(x, dx_f(x))
# plt.grid()
# plt.show()

#Ищем производную f(x) = x^2 - 6x + 9 + 4
#f'(x) = 2x - 6 = 2(x-3)
# строим график производной

L = 0.001 #шаг-скорость обучения

iterations = 100_000

# #x = random.randint(0, 5)
# for i in range(iterations):
#     d_x = dx_f(x)
#     x -= L* d_x
# print(x, f(x))
# x = 3, f(x) = 4

# Градиентный способ работает с линейной регрессией
# y = w_0 + x_1 * x; хотим минимизировать остатки, но сама функция потерь - для каждой линии посчитать остатки и выбрать минимальный
# (x1, y1), ..., (xn, yn)
# E(w_1, w_0) = Сумма Ri ^2 = summ (yi - (w_0 + w_1 * x))^2 -функция потерь
# d/dw_1 * E (w_1, w_0); d/dw_0 * E(w_1, w_0)
# E(w_1,w_0) = summ от i =1 до n (yi^2 - 2yi(w_0 + w_1*x) - (w_0 + w_1*x)^2)=
# = summ от i = 1 до n (yi^2 - 2*yi*w0 - 2*yi*w1*x + w0^2 + 2*w0*w1*x + w1^2*x^2)

#d/d(w0) E(w1,wo) = summ (-2yi + 2w0 + 2w1*x) = 2 summ(-yi + w0 + w1*x)


#d/dw1 E(w1,w0) = summ(-2yi*x + 2w0*x + 2w1x^2) = 2* summ(xi(-yi + w0 + w1*x))

w1 = 0.0
w0 = 0.0
for i in range(iterations):
     D_w0 = 2 * sum((-y[i] + w0 + w1 * x[i]) for i in range(n))
     D_w1 = 2 * sum((x[i] * (-y[i] + w0 + w1 * x[i])) for i in range(n))
     w1 -= L * D_w1
     w0 -= L * D_w0

w1 = np.linspace(-10,10,100)
w0 = np.linspace(-10,10,100)
def E(w1, w0, x, y):
    return sum((y[i] - (w0 + w1*x[i])) ** 2 for i in range(len(x)))

w1, w0 = np.meshgrid(w1, w0)

ew = E(w1,w0,x,y)

fig = plt.figure()
ax = plt.axes(projection="3d")

ax.plot_surface(w1, w0, ew)

w1_fit = 2.4
w0_fit = 0.8

#функция потерь
E_fit = E(w1_fit, w0_fit, x,y)

ax.scatter3D(w1_fit, w0_fit, E_fit, color = 'red')
plt.show()
