#переобучение и дисперсия
# Линейная регрессия минимума из суммы квадратов не гарантирует, что хорошая линейная регрессия вышла
# Цель состоит не в минимизации суммы квадратов, а втом, чтобы сделать правильные предсказания на новых данных
# красный и розовый - сильно переобученные модели, но точно подстраивающие регрессию под обучающие данные

# Переобученные модели очень чувствительны к выбросам, которые находятся далко от остальных точек, в прогнозах будет высокая дисперсия(отклонение от среднего)
# Потому к моделям специально добавляется смещение
# Центральная предельная теорема.
# В модель специально вносят смещение, оно означает, что при попытке построить модель предпочтение отдается определенной схеме (например, должна быть прямая линия, или она должна пройти через (0,0))
# Не график со сложной структурой, минимизирующ сумму остатков
#Если в модель добавить смещение, то есть риск недообучения модели
# Балансировка: минимизация функции потерь(сумма квадратов остатков - мин), с другой стороны - надо вносить смещение чтобы не переобучения не было

#Гребневая регрессия (Ridge) - добавляем смещение в виде штрафа, из-за этого хуже идет подгонка под имеющиеся данные
#Лассо-регрессия, выкидываем признаки, которые не важны -> ведет к удалению некоторых переменных, снижается размерность

# Механически применить линейную регрессию к данным, сделать на основе полученной модели прогноз, после этого считать , что все в порядке нельзя
# Регрессионная модель может и в принципе не подходить

import numpy as np
import pandas as pd
import random
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import matplotlib.pyplot as plt

data = np.array(
    [
        [1, 5],
        [2, 7],
        [3, 7],
        [4, 10],
        [5, 11],
        [6, 14],
        [7, 17],
        [8, 19],
        [9, 22],
        [10, 28],
    ]

)
#Градиентный спуск - пакетный градиентный спуск. Для работы используются ВСЕ доступные обучающие данные
# Стохастический градиентный спуск, на каждой итерации обучаемся только по одной выборке из данных
# Сокращение числа вычислений на практике
# Вносим смещение -> боремся с переобучением - засчет случайности и того, что используем не всю выборку
# Мини-пакеты градиентный спуск, на каждой итерации используется несколько выборок
x= data[:, 0]
y = data[:, 1]

n = len(x)

L = 0.001
#Размер выборки
sample_size = 1
w1 = 0.0
w0 = 0.0

iterations = 100_000
for i in range(iterations):
    idx = np.random.choice(n, sample_size, replace=False)
    D_w0 = 2 * sum(-y[idx] + w0 + w1 * x[idx])
    D_w1 = 2 * sum((x[idx] * (-y[idx] + w0 + w1 * x[idx])))
    w1 -= L * D_w1
    w0 -= L * D_w0

print(w1, w0)

# Как оценить, насколько сильно промахиваются прогнозы при использовании линейной регрессии?
# Для оценки степени взаимосвязи между двумя переменными используем линейный коэффициент корелляции
# Если 0, то нет связи между величинами, -1 - обратная пропорциональность, 1 - пропорциональность
# Тут он вычисляется не так, тк связывает прогнозы с реальными данными, полученными из обучающей выборки

data_df = pd.DataFrame(data)
print(data_df.corr(method="pearson"))

data_df[1] = data_df[1].values[::-1]
print(data_df.corr(method="pearson"))
# Положит значение - чем больше один коэфф тем больше другой, отлиц говорил бы об обратной лин зависимости
# Коэффициент корреляции помогает понять, есть ли связь между двумя переменными, +/-1 - есть связь, ближе к 0 - связи нет

# Как считается коэф корреляции
# r = (n* summ xi*yi - (summ xi * summ yi))/(sqrt(n* summ xi^2 - (summ(xi)^2)* sqrt(n - summyi^2) - (summ yi)^2)

#Обучающие и тестовые выборки - основной метод борьбы с переобучением, заключается в том, что набор данных делится на обучающую и тестовую выборки.abs

# Во всех видах машинного обучения с учителем это деление встречается
# Обычная пропорция это 2/3 на обучение и 1/3 на тест, или 4/5 к 1/5 или 9/10 к 1/10

X = data_df.values[:, :-1]
Y = data_df.values[:, -1]

# print(X)
# print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=1/3)
print(X_train)
print(Y_train)
print(X_test)
print(Y_test)

model = LinearRegression()
model.fit(X_train, Y_train)

r = model.score(X_test, Y_test)
print(r)
#Коэффициент детерминации
# r^2 = 1 - сумма(yi - yi ^)^2/ (сумма yi - yi с чертой)^2
# yi - фактические значения, которые на графике в обучающих данных
# yi^ - предсказанное значение у
# yi с вертикальной чертой - среднее значение для всех y
# чем ближе к 1, тем лучше регрессия работает на тестовых данных, ближе к 0 - хуже
# 0 < r^2 < 1

# Перекрестная валидация - модель обучают трижды и трижды тестируют

kfold = KFold(n_splits=3, random_state=1, shuffle=True) # трехкратная перекрестная валидация
model = LinearRegression()
results = cross_val_score(model,X,Y,cv=kfold)
print(results) #средние квадратические ошибки
print(results.mean(), results.std())

# Метрики - mean и std - показывают, насколько ЕДИНООБРАЗНО ведет себя модель на разных выборках
# Возможно использование поэлементной перекрестной валидации - если данных мало
# Если большая дисперсия - нужна случайная валидация - перемешивает данные, делим пополам

#Валидационная выборка - хотим сравнить работу нескольких моделей или конфигураций

data_df = pd.read_csv('C:\data\multiple_independent_variable_linear.csv')
print(data_df.head())

X = data_df.values[:,:-1]
Y = data_df.values[:,-1]

model = LinearRegression().fit(X,Y)
# y = a0 + a1x1 + a2x2
print(model.coef_, model.intercept_)

x1 = X[:,0]
x2 = X[:, 1]
y=Y

fig = plt.figure()
ax = plt.axes(projection="3d")
ax.scatter3D(x1, x2, y)
x1_ = np.linspace(min(x1), max(x1), 100)
x2_ = np.linspace(min(x2), max(x2), 100)

x1_, x2_ = np.meshgrid(x1_,x2_)

Y_ = model.intercept_ + model.coef_[0]*x1_ + model.coef_[1]*x2_
ax.plot_surface(x1_, x2_, Y_, cmap = "Greys", alpha=0.1)
plt.show()